
def ex_softmax():
    # Implement the softmax function from scratch
    # and compare with the results of tf.nn.softmax.
    pass

def ex_attention():
    # Implement the Attention layer from scratch.
    pass

    # Implement the MultiHeadAttention layer from scratch.
    pass

    # Use one (multihead) attention layer (and possible FFNs)
    # to learn to count the number of times
    # the last letter in a random string appears in the string
    # 'abbca' -> 2 
    # 'abbcb' -> 3
    # 'bbbba' -> 1
    pass

    # Use one self-attention layer (and possible FFNs)
    # to learn to differentiate between
    pass